# AWS
## Module 

1. 
    Which data characteristic describes how accurate, precise, and trusted the data is?

    Velocity

    Variety

    Veracity

    Volume

        answer: Veracity
---

2. 
    Which statements describe volume and velocity in a data pipeline? (Select THREE.)

    Only volume drives the expected throughput and scaling requirements of your pipeline.

    Volume is about how much data you need to process.

    Velocity is about how much data you need to process.

    Velocity is about how quickly data enters and moves through a pipeline.

    Volume and velocity together drive the expected throughput and scaling requirements of a pipeline.

        answer: Volume is about how much data you need to process & Velocity is about how quickly data enters and moves through a pipeline & Volume and velocity together drive the expected throughput and scaling requirements of a pipeline.
---

3. 
    Which data type best describes CSV, JSON, and XML files?

    Unstructured

    Structured

    Semistructured

    Relational

        answer: Semistructured
---

4. 
    Which statement about data types is correct?

    Unstructured data and structured data are equally difficult to query.

    Unstructured data is the hardest to query and the least flexible. Structured data is the easiest to query and the most flexible.

    Unstructured data is the easiest to query but the most flexible. Structured data is the hardest to query but the least flexible.

    Unstructured data is the hardest to query but the most flexible. Structured data is the easiest to query but the least flexible.

        answer: Unstructured data is the hardest to query but the most flexible. Structured data is the easiest to query but the least flexible.
---

5. 
    Which type of data source includes data that is generated continually by events that include a time-based component?

    Public datasets

    On-premises databases

    File stores

    Events, Internet of Things (IoT) devices, and sensors

        answer: Events, Internet of Things (IoT) devices, and sensors
---

6. 
    How could a healthcare company use data to support the most valuable personalized
    experience for their customers?

    Combine disparate data of different types from on-premises databases, public datasets, and customer devices to alert customers who might be at risk for a health event.

    Provide summary reports of health trends based on an internal customer database. The company can ensure the best experience because it has control over this data and is certain of its validity.

    Give customers access to full public health datasets through a mobile app. Users can search for a variety of health conditions.

    Collect heart rate data from customers' personal devices and automatically add it to their healthcare records.

        answer: Combine disparate data of different types from on-premises databases, public datasets, and customer devices to alert customers who might be at risk for a health event.
---

7.
    What is an example of structured data?

    Relational database table

    Clickstream data

    Image

    CSV, JSON, or XML file

        answer: Relational database table
---

8.
    Which scenario could benefit from batch ingestion or processing?

    Real-time alerts are produced based on log data to identify potential fraud as soon as it occurs.
    
    Clickstream data from a retailer's website sends a large volume of small bits of data at a continuous pace. Analysis must be performed immediately.

    A dashboard is populated with real-time error rates of sensors in a factory.

    Sales transaction data from a retailer's website is sent to a central location periodically. Data is analyzed overnight with reports sent to branches in the morning.

        answer: Sales transaction data from a retailer's website is sent to a central location periodically. Data is analyzed overnight with reports sent to branches in the morning.
---

9.
    Which scenario describes a challenge to velocity?

    Data is ingested from regional sales sites, and the overnight batch job fails because it runs out of disk space.

    Regional offices send the same type of data but use different CSV formats for their files.

    A sales department wants to use a data source but does not have information about its lineage or how it has been maintained.

    Clickstream data is collected from a shopping website to make personalized recommendations while a user is shopping. When the website is very busy, there is a delay in returning results to customers.

        answer: Clickstream data is collected from a shopping website to make personalized recommendations while a user is shopping. When the website is very busy, there is a delay in returning results to customers.
---

10.
    Which statement about data veracity is true?

    The first point at which a data engineer or data scientist will address veracity is when cleaning and transforming the data during ingestion.

    The data analytics field shares a common definition of clean data to ensure that veracity standards are met on all data sources.

    Value rests on veracity. If you can't trust the data, then your analysis will not have much value.

    All data issues that challenge veracity can be addressed by cleansing data that enters the pipeline.

        answer: Value rests on veracity. If you can't trust the data, then your analysis will not have much value.
---
